{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the required packages\n",
    "\n",
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>Now we create first class, called Node, which contains some attributes. \n",
    "Most important from them are left and right, that represent child nodes</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"A node in a decision tree.\"\"\"\n",
    "    def __init__(self, X=None, y=None, feature=None,\\\n",
    "                  threshold=None, left=None, right=None, value=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        # only leaf nodes have a value\n",
    "        self.value = value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span>Here's class of DecisionTreeClassifier\n",
    "With method fit you create your tree\n",
    "With method predict you logically predict classes of iris flowers \n",
    "And using method evaluate gives you oppurtunity to see how accurate this prediction is</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionTreeClassifier:\n",
    "    \"\"\"A decision tree classifier.\"\"\"\n",
    "\n",
    "    def __init__(self, max_depth):\n",
    "        self.root = None\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def info_gain(self, y, X, threshold, feature):\n",
    "        '''\n",
    "        Information gain is the difference in entropy before a split\n",
    "        and the weighted average of the entropy after the split.\n",
    "        '''\n",
    "        parent_ent = self.entropy(y)\n",
    "\n",
    "        left = y[X[feature] < threshold]\n",
    "        right = y[X[feature] >= threshold]\n",
    "\n",
    "        left_ent = self.entropy(left)\n",
    "        right_ent = self.entropy(right)\n",
    "\n",
    "        if len(left) == 0 or len(right) == 0:\n",
    "            return 0\n",
    "\n",
    "        n_l, n_r = len(left), len(right)\n",
    "        n = len(y)\n",
    "        child_ent = (n_l/n)* left_ent + (n_r/n)*right_ent\n",
    "\n",
    "        return parent_ent - child_ent\n",
    "\n",
    "\n",
    "    def entropy(self, classes):\n",
    "        '''\n",
    "        Entropy is a measure of the impurity of a set of examples.\n",
    "        The higher the entropy, the more mixed the classes are.\n",
    "        '''\n",
    "        # calculate the frequency of each class\n",
    "        hist = np.bincount(classes)\n",
    "        ps = hist / len(classes)\n",
    "        return -np.sum([p * np.log(p) for p in ps if p>0])\n",
    "\n",
    "\n",
    "    def split_data(self, X, y) -> tuple[int, float, float]:\n",
    "        \"\"\"Find the best feature and threshold to split the data on.\"\"\"\n",
    "\n",
    "        best_split = (0, 0)\n",
    "        features = X.columns\n",
    "        gain = -1\n",
    "        for feature in features:\n",
    "            thresholds = np.unique(X[feature])\n",
    "            for threshold in thresholds:\n",
    "\n",
    "                info_gain = self.info_gain(y, X, threshold, feature)\n",
    "\n",
    "                # if the information gain is greater than the current gain\n",
    "                # update the best split\n",
    "                if info_gain > gain:\n",
    "                    best_split = (feature, threshold)\n",
    "                    gain = info_gain\n",
    "        # return gain as well so I don't have to calculate it again in build_tree\n",
    "        return best_split[0], best_split[1], gain\n",
    "\n",
    "\n",
    "    def build_tree(self, X, y, depth):\n",
    "        \"\"\"Build a decision tree.\"\"\"\n",
    "\n",
    "        if depth <= self.max_depth:\n",
    "            best_split = self.split_data(X, y)\n",
    "            feature, threshold = best_split[:2]\n",
    "            # check whether the ig is greater than 0\n",
    "            if best_split[-1] > 0:\n",
    "                left_subtree = self.build_tree(X[X[feature] < threshold],\\\n",
    "                                                y[X[feature] < threshold], depth + 1)\n",
    "                right_subtree = self.build_tree(X[X[feature] >= threshold],\\\n",
    "                                                 y[X[feature] >= threshold], depth + 1)\n",
    "                return Node(X, y, feature, threshold, left_subtree, right_subtree)\n",
    "\n",
    "        # if the depth is greater than the max depth\n",
    "        # or the information gain is less than 0\n",
    "        # return a leaf node with value of frequency of the most common class\n",
    "        leaf_value = np.argmax(np.bincount(y))\n",
    "        return Node(value=leaf_value)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the decision tree to the data.\"\"\"\n",
    "        X = pd.DataFrame(X)\n",
    "        self.root = self.build_tree(X, y, 0)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Make predictions using the decision tree.\"\"\"\n",
    "\n",
    "        def make_prediction(x, node):\n",
    "            \"\"\"Recursively traverse the tree to make predictions.\"\"\"\n",
    "            # if value is not None, then it's a leaf node\n",
    "            # so we can return the value\n",
    "            if node.value is not None:\n",
    "                return node.value\n",
    "\n",
    "            if x[node.feature] < node.threshold:\n",
    "                return make_prediction(x, node.left)\n",
    "            return make_prediction(x, node.right)\n",
    "\n",
    "        return np.array([make_prediction(x, self.root) for x in X_test])\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"Evaluate the decision tree's accuracy on the test set.\"\"\"\n",
    "\n",
    "        y_pred = self.predict(X_test)\n",
    "        return np.sum(y_pred == y_test) / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9210526315789473"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tree = MyDecisionTreeClassifier(max_depth=1)\n",
    "my_tree.fit(X, y)\n",
    "preds = my_tree.predict(X_test)\n",
    "my_tree.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c9a4a2b301082a8a37ef3b97ef2e69c56daebb1e9b498d28962ff459a0860ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
